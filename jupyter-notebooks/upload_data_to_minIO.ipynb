{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04f79b48-51e6-44ee-ae15-c144cbff36e2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Create Dummy Data for the process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262a828d-7bd7-45ef-af83-f196a1d2e364",
   "metadata": {},
   "source": [
    "## Import modules for Dummy Data Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954196c3-02b2-46e2-823d-b2e862bce9ff",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Adds additional directories for importing custom modules\n",
    "import sys\n",
    "sys.path.append('../generate_dummy_data')\n",
    "\n",
    "from data_generator import generate_dummy_data, write_csv, write_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b0abf0-caa5-4390-adfd-3bb187bb10c8",
   "metadata": {},
   "source": [
    "## Generate dummy data and write to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ae744c-214c-43e9-84f9-8389f835a1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_csv(generate_dummy_data(100), \"../data/dummy_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9b9365-bd7a-4246-86cb-e7eb3c6fc5a1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Generate dummy data and multiple JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966d59e2-fdea-4572-aa1a-c9b86476d956",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_json(generate_dummy_data(100), output_folder=\"../data/json_batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0cfc9b-5765-4a71-8a44-26e3d88be1f4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Uploading files to MinIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7976eead-b1f3-4c63-976d-8426e5c48309",
   "metadata": {},
   "source": [
    "## Pre-requisite for MinIO Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787e68ae-c2b8-4825-a3a7-88189f93f707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adds additional directories for importing custom modules\n",
    "import sys\n",
    "sys.path.append('../file_uploader')\n",
    "\n",
    "from minio import Minio\n",
    "from minio.error import S3Error\n",
    "import os\n",
    "import time\n",
    "from minio_util import get_minio_client, upload_batch_file, upload_json_files\n",
    "\n",
    "# Initialize the MinIO client.\n",
    "client = get_minio_client(endpoint=\"minio:9000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e48e7d1-b051-463d-8367-b7aed67011f1",
   "metadata": {},
   "source": [
    "## Upload a single CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8904b5-2819-4e7f-87cd-a62054ba5a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_source_file = \"../data/dummy_data.csv\"\n",
    "upload_batch_file(client, csv_source_file, \"python-batch-bucket\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea778831-8107-4a1b-9ecb-f57777232cf3",
   "metadata": {},
   "source": [
    "## Upload all JSON batch files from a directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f2ee4c-78b3-4329-bd4b-03b7e3d67550",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "json_directory = \"../data/json_batches\"\n",
    "upload_json_files(client, json_directory, \"python-process-bucket\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1b47a8-2cc9-4d40-91de-eb655ab6f14f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Process batch file with Spark into Delta Lake Format and Saving in MinIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b23d9b-6184-4bea-8bd4-486902d06e41",
   "metadata": {},
   "source": [
    "## Pre-requisites for reading Batch file in MinIO and converting it to Delta Lake using PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a944e9c-e479-4bcf-a328-d21abfc53ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta import *\n",
    "\n",
    "# Set up SparkSession with Delta and MinIO\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DeltaLakeOnMinIO\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "hadoop_conf = spark._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.s3a.endpoint\", \"http://minio:9000\")  # use the Docker service name or IP\n",
    "hadoop_conf.set(\"fs.s3a.access.key\", \"ROOTNAME\")\n",
    "hadoop_conf.set(\"fs.s3a.secret.key\", \"CHANGEME123\")\n",
    "hadoop_conf.set(\"fs.s3a.path.style.access\", \"true\")  # Required for MinIO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0234fb0-3925-4798-bd63-673c9438d4df",
   "metadata": {},
   "source": [
    "## Read CSV batch file from MinIO \n",
    "> \"**minio_csv_batch_file_full_path**\" value, may need to be manually updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75723c7-5497-4cd2-9e0e-b91ffd6cea41",
   "metadata": {},
   "outputs": [],
   "source": [
    "minio_csv_batch_file_full_path = \"s3a://python-batch-bucket/1741409275634_dummy_data.csv\"\n",
    "df = spark.read\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"delimiter\", \",\")\\\n",
    "    .option(\"ignoreLeadingWhiteSpace\", \"true\")\\\n",
    "    .option(\"ignoreTrailingWhiteSpace\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .csv(minio_csv_batch_file_full_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e50a65-696f-4c9b-ac66-79dff55fd2ea",
   "metadata": {},
   "source": [
    "## Data Exploration (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100236e2-a994-4683-b48b-9bce98747c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37cc2e3-5820-4cd5-9e02-5ac44ae4e9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f8be00-6553-4376-9e05-e7171444df91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b7ae9a-8fc1-4bc1-9a72-7e43ae9fd133",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b767d0-c56a-41b7-9eac-fa427c0f4d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill null values with a specified value\n",
    "df_clean = df.na.fill({\"is_active\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0662fa77-c382-47a0-9196-cba5b1ff29bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with any null values\n",
    "df_clean = df_clean.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c046809-f7dd-4c73-8756-ca137b28e46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter based upon Salary\n",
    "df_clean = df_clean.filter(df_clean[\"salary\"] > 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a0a043-74a1-4aa3-a6d4-26f49e4f2765",
   "metadata": {},
   "source": [
    "### Overwriting variable to re-use with data exploration segment (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada15f42-4c5c-44d9-a6e0-14fcf169c857",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe5bf94-7a34-4b80-a1dc-5ce128eb0013",
   "metadata": {},
   "source": [
    "## Writes DataFrame in Delta Lake format to MinIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19081053-0bc4-4810-b39f-dd839052ce6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.write.format(\"delta\").mode(\"overwrite\").save(\"s3a://python-batch-bucket/delta_output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4db213a-eb86-4580-a0eb-22aa953da5ff",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Process JSON Stream with Spark into Delta Lake Format and Saving in MinIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b450c309-0527-4430-955d-0d9af08fbb0f",
   "metadata": {},
   "source": [
    "## Pre-requisites for reading Stream in MinIO and converting them to Delta Lake using PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574d427a-b0ee-460c-98da-a621abfa1e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adds additional directories for importing custom modules\n",
    "import sys\n",
    "sys.path.append('../file_uploader')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType, DoubleType, BooleanType\n",
    "from delta import *\n",
    "from minio_util import get_minio_client, ensure_bucket\n",
    "\n",
    "# Initialize the MinIO client.\n",
    "client = get_minio_client(endpoint=\"minio:9000\")\n",
    "ensure_bucket(client, \"checkpoints\")\n",
    "\n",
    "# Set up SparkSession with Delta and MinIO\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DeltaLakeOnMinIO\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "hadoop_conf = spark._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.s3a.endpoint\", \"http://minio:9000\")  # use the Docker service name or IP\n",
    "hadoop_conf.set(\"fs.s3a.access.key\", \"ROOTNAME\")\n",
    "hadoop_conf.set(\"fs.s3a.secret.key\", \"CHANGEME123\")\n",
    "hadoop_conf.set(\"fs.s3a.path.style.access\", \"true\")  # Required for MinIO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8afc970-0767-4d1e-a5f0-9247fb1dd093",
   "metadata": {},
   "source": [
    "## Listening to Directory to process Stream (Interrupt the kernel using GUI to stop this process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7fa896-e05b-4450-8cdd-7a6e79770e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory in MinIO where new JSON files will arrive\n",
    "input_path = \"s3a://python-process-bucket/\"\n",
    "\n",
    "# Defining schema\n",
    "schema = StructType() \\\n",
    "    .add(\"id\", \"integer\") \\\n",
    "    .add(\"name\", \"string\") \\\n",
    "    .add(\"email\", \"string\") \\\n",
    "    .add(\"address\", \"string\") \\\n",
    "    .add(\"phone\", \"string\") \\\n",
    "    .add(\"date_of_birth\", \"date\") \\\n",
    "    .add(\"company\", \"string\") \\\n",
    "    .add(\"salary\", \"double\") \\\n",
    "    .add(\"is_active\", \"boolean\")\n",
    "\n",
    "# Read the stream of JSON files as they are added\n",
    "df_stream = spark.readStream \\\n",
    "    .schema(schema) \\\n",
    "    .option(\"multiline\",\"true\") \\\n",
    "    .json(input_path)\n",
    "\n",
    "# Process/Clean the streaming DataFrame\n",
    "df_transformed = df_stream\n",
    "# Fill null values with a specified value\n",
    "df_transformed = df_transformed.na.fill({\"is_active\": False})\n",
    "# Drop rows with any null values\n",
    "df_transformed = df_transformed.na.drop()\n",
    "# Filter based upon Salary\n",
    "df_transformed = df_transformed.filter(df_transformed[\"salary\"] > 10000)\n",
    "\n",
    "# Write the processed stream to Delta Lake format on MinIO.\n",
    "# Note: A checkpoint location is required to track progress.\n",
    "output_path = \"s3a://python-process-bucket/delta_output\"\n",
    "checkpoint_path = \"s3a://checkpoints/delta_streaming\"\n",
    "\n",
    "query = df_stream.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", checkpoint_path) \\\n",
    "    .start(output_path)\n",
    "\n",
    "console_query = df_stream.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410429f6-da1b-4102-9e9b-50b2dae862e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()\n",
    "console_query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104793b2-437a-410b-aa2a-342e889b1660",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Data Warehouse using Delta Lake files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed658db-e3c8-4a8a-923e-acf71ce80887",
   "metadata": {},
   "source": [
    "## Pre-requisites for Reading Delta Lakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029410c3-59e1-4656-b160-623d13c9b330",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta import *\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DeltaLakeOnMinIO\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "hadoop_conf = spark._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.s3a.endpoint\", \"http://minio:9000\")  # use the Docker service name or IP\n",
    "hadoop_conf.set(\"fs.s3a.access.key\", \"ROOTNAME\")\n",
    "hadoop_conf.set(\"fs.s3a.secret.key\", \"CHANGEME123\")\n",
    "hadoop_conf.set(\"fs.s3a.path.style.access\", \"true\")  # Required for MinIO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edafe3f5-5775-4fea-9841-457f731129ea",
   "metadata": {},
   "source": [
    "## Reading from python-process-bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e212f3-f540-4557-ab40-5b48aec264a3",
   "metadata": {},
   "source": [
    "### Create Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8d455a-aed3-4dc4-93a1-ff2c2f97878c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS json_stream_table\n",
    "    USING DELTA\n",
    "    LOCATION 's3a://python-process-bucket/delta_output'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1c5b27-8a94-45c6-b57d-43be214fc245",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM json_stream_table limit 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6bcfd6-ee16-48fa-a8ac-fb4dab652729",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check = spark.read.format(\"delta\").load(\"s3a://python-process-bucket/delta_output\")\n",
    "df_check.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff87cf6b-3b20-43bd-adbf-9cd9649790fb",
   "metadata": {},
   "source": [
    "## Reading from python-batch-bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5c1e74-9cfc-4784-b745-1f5540714059",
   "metadata": {},
   "source": [
    "### Create Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd93911-33e3-45fe-b702-ba624d49ef5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS csv_batch_table\n",
    "    USING DELTA\n",
    "    LOCATION 's3a://python-batch-bucket/delta_output'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f7af6a-3a26-48d9-860a-f7e01f942822",
   "metadata": {},
   "source": [
    "### Read from csv_batch_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f873d382-8811-4a48-8622-e52f41e61c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM csv_batch_table limit 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce892a46-8602-4bef-99b7-98e5bef6c9b9",
   "metadata": {},
   "source": [
    "### Directly read Delta Lake file by loading it into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ccb246-f926-4bdf-9ef7-7f81712a5f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check = spark.read.format(\"delta\").load(\"s3a://python-batch-bucket/delta_output\")\n",
    "df_check.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5f261e-b8e5-40e2-a9d3-fda26aae76a9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Delete MinIO Object(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f570541c-ee62-4355-9c36-b53232db36c8",
   "metadata": {},
   "source": [
    "## Remove object\n",
    "https://github.com/minio/minio-py/blob/88f4244fe89fb9f23de4f183bdf79524c712deaa/examples/remove_object.py#L25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebe2488-0b1f-45c8-9898-805175a9c082",
   "metadata": {},
   "source": [
    "### Import modules for MinIO deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0799a5bf-f9e9-47e6-8daf-4ec5dd7fe1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minio import Minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4c7843-682c-4a92-897c-c2b8b3278d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.remove_object(\"python-batch-bucket\",\"file_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dfad7c-6cd5-4441-905f-ef3f144ea1d1",
   "metadata": {},
   "source": [
    "## Remove a prefix recursively\n",
    "https://github.com/minio/minio-py/blob/88f4244fe89fb9f23de4f183bdf79524c712deaa/examples/remove_objects.py#L38"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84370ae-aa3f-463a-8ac0-e9986bb03635",
   "metadata": {},
   "source": [
    "### Import modules for MinIO multi-deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec8abe8-0724-4fc3-ac5b-579a7b60d768",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minio import Minio\n",
    "from minio.deleteobjects import DeleteObject"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81800000-4e9e-406c-8934-f4b219e6b3a2",
   "metadata": {},
   "source": [
    "### Delete file(s) with prefix (Prefix = full directory path to delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa52681-169a-4f6a-90da-b382fb6b8be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_object_list = map(\n",
    "    lambda x: DeleteObject(x.object_name),\n",
    "    client.list_objects(\"python-batch-bucket\", \"delta_output/\", recursive=True),\n",
    ")\n",
    "errors = client.remove_objects(\"python-batch-bucket\", delete_object_list)\n",
    "for error in errors:\n",
    "    print(\"error occurred when deleting object\", error)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
